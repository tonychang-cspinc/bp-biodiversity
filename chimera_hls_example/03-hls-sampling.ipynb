{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Chimera Training Samples\n",
    "\n",
    "Once tiles have been collected in notebook two, we can use the FIA catalog generated in notebook one to sample data by point location and produce \"chips\" used for training. Each point in the catalog is associated with ground truth data collected by FIA. \n",
    "\n",
    "**Note: Some indexerrors are expected for larger chip sizes as only points are checked for intersection and samples may spans two tiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import fiona.transform\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "from affine import Affine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    from dask_gateway import GatewayCluster\n",
    "    clustenv='distributed'\n",
    "except ModuleNotFoundError:\n",
    "    clustenv='local'\n",
    "    print('Using a local cluster...')\n",
    "\n",
    "from utils.hls import catalog\n",
    "from utils.hls import compute\n",
    "from utils import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger('hls-point-sampling')\n",
    "cluster_args = dict(\n",
    "    workers=64, #int(.8*os.cpu_count()),\n",
    "    worker_threads=3,\n",
    "    worker_memory=16,\n",
    "    scheduler_threads=4,\n",
    "    scheduler_memory=8,\n",
    "    clust_type=clustenv\n",
    ")\n",
    "code_path = '../utils'\n",
    "checkpoint_path = 'checkpoints/sampling.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with your desired blob container for sample data collection\n",
    "%store -r envdict\n",
    "envdict['CHIP_BLOB_CONTAINER'] = ''\n",
    "envdict['COL_ENV'] = clustenv\n",
    "%store envdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_path = fsspec.get_mapper(\n",
    "    f\"az://fia/catalogs/fia_tiles.zarr\",\n",
    "    account_name=envdict['AZURE_STRG_ACCOUNT_NAME'],\n",
    "    account_key=envdict['AZURE_STRG_ACCOUNT_KEY']\n",
    ")\n",
    "pt_catalog = catalog.HLSCatalog.from_zarr(catalog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_pts = pt_catalog.xr_ds.where(pt_catalog.xr_ds['year'] >= 2019, drop=True)\n",
    "hls_pts = hls_pts.to_dataframe()\n",
    "jobs = hls_pts.groupby(['tile', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttdf = pd.read_csv('test_data/fd_test_tiles.csv')\n",
    "test_tiles = ttdf['tile'].values\n",
    "\n",
    "test_jobs = [job for job in jobs if job[0][0] in test_tiles]\n",
    "col_inds = []\n",
    "for ji, j in enumerate(test_jobs):\n",
    "    ttdfrow=ttdf[ttdf['tile'] == j[0][0]].iloc[0]\n",
    "    if j[0][1] == ttdfrow.year:\n",
    "        col_inds.append(ji)\n",
    "test_jobs = list(map(test_jobs.__getitem__,col_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import get_worker\n",
    "\n",
    "def chip(ds, lat, lon, chip_size, metadata):\n",
    "    CRS = \"EPSG:4326\"\n",
    "    tfm = Affine(*ds.attrs['transform'])\n",
    "    ([x], [y]) = fiona.transform.transform(\n",
    "        CRS, ds.attrs['crs'], [lon], [lat]\n",
    "    )\n",
    "    x_idx, y_idx = [round(coord) for coord in ~tfm * (x, y)]\n",
    "\n",
    "    half_chip = int(chip_size/2)\n",
    "    try:\n",
    "        return ds[dict(x=range(x_idx-half_chip, x_idx+half_chip), y=range(y_idx-half_chip, y_idx+half_chip))]\n",
    "    except IndexError:\n",
    "        get_worker().log_event(\"message\", {\"type\": \"IndexError\", **metadata})\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "def chip_tile_year(\n",
    "    job_id, job_df, chip_size, bands, account_name, chip_container, tile_container, account_key\n",
    "):\n",
    "    def sample_and_write(tl, row):\n",
    "        sample = chip(\n",
    "            tl,\n",
    "            row['lat'],\n",
    "            row['lon'],\n",
    "            chip_size,\n",
    "            metadata={'index': row['INDEX'], 'tile': row['tile'], 'year': row['year']}\n",
    "        )\n",
    "        if sample:\n",
    "            output_zarr = fsspec.get_mapper(\n",
    "                f\"az://{chip_container}/{int(row['INDEX'])}-{row['tile']}.zarr\",\n",
    "                account_name=account_name,\n",
    "                account_key=account_key\n",
    "            )\n",
    "            sample.chunk({'month': 12, 'x': 32, 'y': 32}).to_zarr(output_zarr, mode='w')\n",
    "    band_names = [band.name for band in bands]\n",
    "    tile, year = job_id\n",
    "    input_zarr = fsspec.get_mapper(\n",
    "        f\"az://{tile_container}/{float(year)}/{tile}.zarr\",\n",
    "        account_name=account_name,\n",
    "        account_key=account_key\n",
    "    )\n",
    "    try:\n",
    "        ds = xr.open_zarr(input_zarr)[band_names].persist()\n",
    "    except:\n",
    "        errstr = f\"az://{tile_container}/{float(year)}/{tile}.zarr\"\n",
    "        raise ValueError(errstr)\n",
    "    samples = []\n",
    "    for _, row in job_df.iterrows():\n",
    "        samples.append(sample_and_write(ds, row))\n",
    "    return job_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute.process_jobs(\n",
    "    jobs=test_jobs,\n",
    "    job_fn=chip_tile_year,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    logger=logger,\n",
    "    cluster_args=cluster_args,\n",
    "    code_path=code_path,\n",
    "    concurrency=6,  # run 6 jobs at once\n",
    "    cluster_restart_freq=42,  # restart after 42 jobs\n",
    "    # chip_tile_year kwargs\n",
    "    bands=pt_catalog.xr_ds.attrs['bands'],\n",
    "    chip_size=32,\n",
    "    account_name=envdict['AZURE_STRG_ACCOUNT_NAME'],\n",
    "    chip_container=envdict['CHIP_BLOB_CONTAINER'],\n",
    "    tile_container=envdict['TILE_BLOB_CONTAINER'],\n",
    "    account_key=envdict['AZURE_STRG_ACCOUNT_KEY']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
