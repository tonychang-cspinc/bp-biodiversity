{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLS Monthly Median Collection\n",
    "\n",
    "This notebook demonstrates the collection of monthly median HLS tiles given a catalog generated in the first [notebook](01-hls-catalog.ipynb), or any catalog generated with the utils.hls.HLSCatalog class methods. Dask is utilized for working with excessively large files that do not fit in RAM. While local computing will function in the given Anaconda environment, scaling with Microsoft's Planetary Computer, or (even better) an equivalent environment in the East US 2 region (where HLS data is stored) is recommended. It is also recommended to monitor both log outputs and blob storage during collection to ensure that tiles are being continuously collected and stored without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# pip/conda installed\n",
    "import fsspec\n",
    "try:\n",
    "    from dask_gateway import GatewayCluster\n",
    "    clustenv='distributed'\n",
    "except ModuleNotFoundError:\n",
    "    clustenv='local'\n",
    "    print('Using a local cluster...')\n",
    "\n",
    "from utils import get_logger\n",
    "from utils.hls.catalog import HLSCatalog\n",
    "from utils.hls.compute import process_jobs\n",
    "from utils.hls.compute import jobs_from_catalog\n",
    "from utils.hls.compute import calculate_job_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'envdict' (dict)\n"
     ]
    }
   ],
   "source": [
    "# fill with your desired blob container for tile data collection\n",
    "%store -r envdict\n",
    "envdict['TILE_BLOB_CONTAINER'] = ''\n",
    "envdict['COL_ENV'] = clustenv\n",
    "%store envdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a local cluster is being used, only the workers and clust_type arguments are used and resources are allocated dynamically.\n",
    "logger = get_logger('hls-test')\n",
    "cluster_args = dict(\n",
    "    workers=128, #int(.8*os.cpu_count())\n",
    "    clust_type=envdict['COL_ENV'],\n",
    "    worker_threads=1,\n",
    "    worker_memory=8,\n",
    "    scheduler_threads=4,\n",
    "    scheduler_memory=8\n",
    ")\n",
    "code_path = '../utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = fsspec.get_mapper(\n",
    "    f\"az://{envdict['CATALOG_BLOB_CONTAINER']}/catalogs/hls_test_tiles.zarr\",\n",
    "    account_name=\"usfs\",\n",
    "    account_key=envdict['AZURE_STRG_ACCOUNT_KEY'] \n",
    ")\n",
    "catalog = HLSCatalog.from_zarr(catalog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs for calculate_job_median\n",
    "job_groupby = \"time.month\"\n",
    "bands = catalog.xr_ds.attrs['bands']\n",
    "chunks = {'band': 1, 'x': 3660, 'y': 3660} # read an entire tile once (each tile is 3660x3660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to scenes from 2015 and later, then group by year\n",
    "yr_catalogs = catalog.xr_ds.where(catalog.xr_ds['year'] >= 2015, drop=True).groupby('year')\n",
    "catalog_groupby = \"tile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for yr, ca in yr_catalogs:\n",
    "    logger.info(f\"Starting process for {yr}\")\n",
    "    ca.info()\n",
    "    storage_prefix = f\"{envdict['TILE_BLOB_CONTAINER']}/{yr}\"\n",
    "    if not os.path.exists('checkpoints/'):\n",
    "        os.makedirs('checkpoints')\n",
    "    checkpoint_path = f\"checkpoints/{yr}.txt\"\n",
    "    jobs = jobs_from_catalog(ca, catalog_groupby)\n",
    "    process_jobs(\n",
    "        jobs=jobs,\n",
    "        job_fn=calculate_job_median,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logger=logger,\n",
    "        cluster_args=cluster_args,\n",
    "        code_path=code_path,\n",
    "        concurrency=2,  # run 2 jobs at once\n",
    "        cluster_restart_freq=16,  # restart after 16 jobs\n",
    "        # kwargs for calculate_job_median\n",
    "        job_groupby=job_groupby,\n",
    "        bands=bands, \n",
    "        chunks=chunks,\n",
    "        account_name=envdict['AZURE_STRG_ACCOUNT_NAME'],\n",
    "        storage_container=storage_prefix,\n",
    "        account_key=envdict['AZURE_STRG_ACCOUNT_KEY'],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
